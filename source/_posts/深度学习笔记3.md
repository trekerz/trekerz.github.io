---
title: 深度学习笔记3
subtitle: "【笔记】稀疏编码"
date: 2017-03-24 13:22:11
tags: 
	- 笔记
	- 读书笔记
	- 深度学习
layout: post
author: "Trekerz"
header-img: "170324.jpg"
---

### 

### **1.    概念**

如果把“输出必须和输入相等”的限制放松，同时利用基的概念，即O=a<sub>1</sub>\*Φ<sub>1</sub>+a<sub>2</sub>\*Φ<sub>2</sub>+...+a<sub>n</sub>\*Φ<sub>n</sub>，Φ<sub>i</sub>是基，a<sub>i</sub>是系数，我们就可以得到这样一个优化问题：Min|I-O|，其中I表示输入，O表示输出。

通过求解这个最优化式子，可以求得系数和基，这些系数和基就是输入I的另一种近似表达：

![img](1.png)

如果我们在上述式子加上L1的Regularity限制，就得到：

![img](2.png)

 

**稀疏编码**，就是将一个信号表示为一组基的线性组合，而且要求只需要较少的几个基就可以将信号表示出来。

**稀疏性**：<u>只有很少的几个非零元素</u>或<u>只有很少的几个远大于零的元素</u>。

### **2.    稀疏编码算法**

无监督学习，用来寻找一组“超完备”基向量（基向量个数比输入向量的维数要大）来更高效地表示样本数据。

超完备基的好处是它们能更有效地找出隐含在输入数据内部的结构与模式。

![img](3.png)

稀疏编码分为两个部分：

a.    训练阶段：给定一系列样本图片[x<sub>1</sub>,x<sub>2</sub>,...]，我们需要学习得到一组基[Φ<sub>1</sub>,Φ<sub>2</sub>,...]，也就是字典。

​         稀疏编码是k-means算法的变体，其训练过程差不多（EM算法的思想：如果要优化的目标函数包含两个变量，如L(W,B)，那么我们可以先固定W，调整B使得L最小，然后再固定B，调整W使L最小，这样迭代交替，不断将L推向最小值。）

训练过程就是一个重复迭代的过程，交替更改a和Φ使得下面的目标函数最小：

![img](4.png)

每次迭代分两步：

1)     固定字典Φ[k]，然后调整a[k]，使得上式最小（即解LASSO问题）

2)     然后固定a[k]，调整Φ[k]，使得上式最小（即解凸QP问题）

​         不断迭代，直至收敛，这样就可以得到一组可以良好表示这一系列x的基，也就是字典。

b.    编码阶段

给定一个新的图片x，由上面得到的字典，通过解一个LASSO问题得到稀疏向量a。这个稀疏向量就是这个输入向量x的一个稀疏表达了。

![img](5.png)

例如：

![img](6.png)

<br/>

<br/>

 