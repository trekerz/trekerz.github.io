---
title: LDA算法简介
subtitle: "【笔记】文档主题生成模型算法"
date: 2017-01-05 10:34:11
tags: 
	- 笔记
	- 算法
	- 推荐系统
layout: post
author: "Trekerz"
header-img: "/bing/BigWindDay_ZH-CN1837859776_1920x1080.jpg"
---



LDA（Latent Dirichlet Allocation）是一种文档主题生成模型，也称为一个**三层贝叶斯概率模型**，**包含词、主题和文档三层结构**。

<br/>

所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“**以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语**”这样一个过程得到。文档到主题服从多项式分布，主题到词服从多项式分布。

<br/>

LDA是一种非监督机器学习技术。它采用了词袋（bag of words）的方法，这种方法将每一篇文档视为一个词频向量，从而将文本信息转化为了易于建模的数字信息。

但是词袋方法没有考虑词与词之间的顺序，这简化了问题的复杂性，同时也为模型的改进提供了契机。

每一篇文档代表了一些主题所4构成的一个概率分布，而每一个主题又代表了很多单词所构成的一个概率分布。



对于语料库中的每篇文档，LDA定义了如下生成过程（generativeprocess）：

1. 对每一篇文档，从主题分布中抽取一个主题；
2. 从上述被抽到的主题所对应的单词分布中抽取一个单词；
3. 重复上述过程直至遍历文档中的每一个单词。

语料库中的每一篇文档与T（通过反复试验等方法事先给定）个主题的一个多项分布 （multinomialdistribution）相对应，将该多项分布记为θ。每个主题又与词汇表（vocabulary）中的V个单词的一个多项分布相对应，将这个多项分布记为φ。

<br/>

**LDA整体流程**：

先定义一些字母的含义：文档集合D，主题（topic)集合T

D中每个文档d看作一个单词序列<w1,w2,...,wn>，wi表示第i个单词，设d有n个单词。（LDA里面称之为wordbag，实际上每个单词的出现位置对LDA算法无影响）

·D中涉及的所有不同单词组成一个大集合VOCABULARY（简称VOC），LDA以文档集合D作为输入，希望训练出的两个结果向量（设聚成k个topic，VOC中共包含m个词）：

·对每个D中的文档d，对应到不同Topic的概率θd<pt1,...,ptk>，其中，pti表示d对应T中第i个topic的概率。计算方法是直观的，pti=nti/n，其中nti表示d中对应第i个topic的词的数目，n是d中所有词的总数。

·对每个T中的topict，生成不同单词的概率φt<pw1,...,pwm>，其中，pwi表示t生成VOC中第i个单词的概率。计算方法同样很直观，pwi=Nwi/N，其中Nwi表示对应到topict的VOC中第i个单词的数目，N表示所有对应到topict的单词总数。

<br/>

**LDA的核心公式如下**：

p(w|d)=p(w|t)*p(t|d)

直观的看这个公式，就是以Topic作为中间层，可以通过当前的θd和φt给出了文档d中出现单词w的概率。其中p(t|d)利用θd计算得到，p(w|t)利用φt计算得到。

实际上，利用当前的θd和φt，我们可以为一个文档中的一个单词计算它对应任意一个Topic时的p(w|d)，然后根据这些结果来更新这个词应该对应的topic。然后，如果这个更新改变了这个单词所对应的Topic，就会反过来影响θd和φt。

<br/>